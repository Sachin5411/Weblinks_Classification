{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from flashtext.keyword import KeywordProcessor\n",
    "from nltk import tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Managing given keywords\n",
    "with open('Interest groups - 5th Sept.txt','r') as f:\n",
    "    c=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[0]=c[0][3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords=np.array(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(keywords)):\n",
    "    if i == len(keywords)-1:\n",
    "        keywords[i]=keywords[i]\n",
    "    else:\n",
    "        keywords[i]=keywords[i][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads All sites file\n",
    "with open('Quantcast-Top-Million.csv.txt','r') as f:\n",
    "    sites=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weblinks=[]\n",
    "for i in range(6000,7100):\n",
    "    j=sites[i][5:-1]\n",
    "    if j=='Hidden profile':\n",
    "        pass\n",
    "    else:\n",
    "        weblinks.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(weblinks)):\n",
    "    weblinks[i]='https://www.'+weblinks[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample\n",
    "weblinks[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving links in database\n",
    "\n",
    "conn= mysql.connector.connect(host='localhost',user='sachin',passwd='mysqlsachin',database='interntask2')\n",
    "\n",
    "mycursor=conn.cursor()\n",
    "\n",
    "mycursor.execute(\"\"\"DROP TABLE IF EXISTS weblink\"\"\")\n",
    "mycursor.execute(\"CREATE TABLE weblinks(weblinks VARCHAR(800))\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "query = \"INSERT INTO randomlinks (weblinks) VALUES (%s)\"\n",
    "mycursor.executemany(query, [(list_item, ) for list_item in weblinks])\n",
    "\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weblinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp0=KeywordProcessor()\n",
    "\n",
    "for word in keywords:\n",
    "    kp0.add_keyword(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gethtmlcontent(url):\n",
    "    try:\n",
    "        soup = BeautifulSoup(url, 'html.parser')  #Parse html code\n",
    "        \n",
    "        texts = soup.findAll(text=True)                 #find all text\n",
    "        text_from_html=' '.join(texts)                   #join all text\n",
    "    \n",
    "    \n",
    "    except Exception as error:\n",
    "        print('Error occured: {}'.format(error))\n",
    "        return []\n",
    "    return text_from_html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=stopwords.words('english')\n",
    "to_remove=['[]','','1','()','||','=','.',',','\\n',':',';','\\\\','//','/']\n",
    "\n",
    "for i in to_remove:\n",
    "    sw.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_keywords(text):\n",
    "    x=str(text)\n",
    "    \n",
    "    x=x.split(' ')\n",
    "    words=[w for w in x]\n",
    "    sws=set(sw)\n",
    "    useful_words=[w for w in words if w not in sws]\n",
    "    \n",
    "    uniq=np.unique(useful_words,return_counts=True)\n",
    "    \n",
    "    if len(uniq[1])==0:\n",
    "        return 0\n",
    "    else:\n",
    "        index=np.argmax(uniq[1])\n",
    "    \n",
    "    \n",
    "    return uniq[0][index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_class(text_from_html):\n",
    "    x=str(text_from_html)\n",
    "    y0 = len(kp0.extract_keywords(x))\n",
    "    if y0==0:\n",
    "        top_kw=top_keywords(text_from_html)\n",
    "        #Category='None'\n",
    "        return 0,top_kw\n",
    "    \n",
    "    else:\n",
    "        kw=kp0.extract_keywords(x)\n",
    "        uniq_kw=np.unique(kw,return_counts=True)\n",
    "        m=np.argmax(uniq_kw[1])\n",
    "        max_kw=uniq_kw[1][m]\n",
    "        #y0 = len(kp0.extract_keywords(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return (max_kw/len(keywords))*100,kw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confidences=[]\n",
    "category=[]\n",
    "links=[]\n",
    "combine=[]\n",
    "for i in range(len(weblinks)):\n",
    "    url= weblinks[i]\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Cafari/537.36'}\n",
    "    \n",
    "\n",
    "    try:\n",
    "        page = requests.get(url,headers=headers)       #to extract page from website\n",
    "    except requests.exceptions.ConnectionError:\n",
    "    \n",
    "        i+=1\n",
    "        continue\n",
    "    sc=page.status_code\n",
    "    if sc == 403:\n",
    "        continue\n",
    "    \n",
    "    html_code = page.content       #to extract html code from page\n",
    "    text_from_html=gethtmlcontent(html_code)\n",
    "    \n",
    "    confidence,kw=find_class(text_from_html)\n",
    "    confidences.append(confidence)\n",
    "    \n",
    "    print(\"Confidience : \",confidence)\n",
    "    uniqkw=np.unique(kw,return_counts=True)\n",
    "    \n",
    "    m=np.argmax(uniqkw[1])\n",
    "    links.append(weblinks[i])\n",
    "    \n",
    "    print(\"Top_keyword: \",uniqkw[0][m])\n",
    "    #print(i)\n",
    "    print(\"Link: \",weblinks[i])\n",
    "    category.append(uniqkw[0][m])\n",
    "    combine.append((weblinks[i],confidence,uniqkw[0][m]))\n",
    "    print('-'*25)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined=np.array(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn= mysql.connector.connect(host='localhost',user='sachin',passwd='mysqlsachin',database='atginterntask2')\n",
    "\n",
    "mycursor=conn.cursor()\n",
    "\n",
    "mycursor.execute(\"\"\"DROP TABLE IF EXISTS web_classification\"\"\")\n",
    "mycursor.execute(\"CREATE TABLE web_classification(weblinks VARCHAR(800),confidence INT(200),kw VARCHAR(600))\")\n",
    "\n",
    "\n",
    "query = \"\"\"INSERT INTO  web_classification(Links,Confidence,Top_Keyword) \n",
    "           VALUES (%s %s %s)\"\"\"\n",
    "\n",
    "mycursor.executemany(query, combined.all)\n",
    "                     \n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
